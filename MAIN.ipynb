{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8499ea4c-5096-42eb-801d-0fbbd280ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback 1: 'The product is amazing! I love the quality'\n",
      "Tokens:  ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality']\n",
      "Lemmas:  ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality']\n",
      "Dependencies:\n",
      "The -> det (head: product)\n",
      "product -> nsubj (head: is)\n",
      "is -> ROOT (head: is)\n",
      "amazing -> acomp (head: is)\n",
      "! -> punct (head: is)\n",
      "I -> nsubj (head: love)\n",
      "love -> ROOT (head: love)\n",
      "the -> det (head: quality)\n",
      "quality -> dobj (head: love)\n",
      "\n",
      "Feedback 2: 'The customer service is terrible, very bad'\n",
      "Tokens:  ['The', 'customer', 'service', 'is', 'terrible', ',', 'very', 'bad']\n",
      "Lemmas:  ['the', 'customer', 'service', 'be', 'terrible', ',', 'very', 'bad']\n",
      "Dependencies:\n",
      "The -> det (head: service)\n",
      "customer -> compound (head: service)\n",
      "service -> nsubj (head: is)\n",
      "is -> ROOT (head: is)\n",
      "terrible -> acomp (head: is)\n",
      ", -> punct (head: bad)\n",
      "very -> advmod (head: bad)\n",
      "bad -> acomp (head: is)\n",
      "\n",
      "Feedback 3: 'Great experience overall, highly recommended.'\n",
      "Tokens:  ['Great', 'experience', 'overall', ',', 'highly', 'recommended', '.']\n",
      "Lemmas:  ['great', 'experience', 'overall', ',', 'highly', 'recommend', '.']\n",
      "Dependencies:\n",
      "Great -> amod (head: experience)\n",
      "experience -> nsubj (head: recommended)\n",
      "overall -> advmod (head: recommended)\n",
      ", -> punct (head: recommended)\n",
      "highly -> advmod (head: recommended)\n",
      "recommended -> ROOT (head: recommended)\n",
      ". -> punct (head: recommended)\n",
      "\n",
      "Feedback 4: 'The delivery was late, very frustrating.'\n",
      "Tokens:  ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']\n",
      "Lemmas:  ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']\n",
      "Dependencies:\n",
      "The -> det (head: delivery)\n",
      "delivery -> nsubj (head: was)\n",
      "was -> ROOT (head: was)\n",
      "late -> advmod (head: frustrating)\n",
      ", -> punct (head: frustrating)\n",
      "very -> advmod (head: frustrating)\n",
      "frustrating -> acomp (head: was)\n",
      ". -> punct (head: was)\n"
     ]
    }
   ],
   "source": [
    "#Case Study 1 (Word Analysis)\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "customer_feedback = [\n",
    "    \"The product is amazing! I love the quality\",\n",
    "    \"The customer service is terrible, very bad\",\n",
    "    \"Great experience overall, highly recommended.\",\n",
    "    \"The delivery was late, very frustrating.\"\n",
    "]\n",
    "\n",
    "def analyze_feedback(feedback):\n",
    "    for idx, text in enumerate(feedback, start=1):\n",
    "        doc = nlp(text)\n",
    "        print(f\"\\nFeedback {idx}: '{text}'\")\n",
    "        print(\"Tokens: \", [token.text for token in doc])\n",
    "        print(\"Lemmas: \", [token.lemma_ for token in doc])\n",
    "        print(\"Dependencies:\")\n",
    "        for token in doc:\n",
    "            print(f\"{token.text} -> {token.dep_} (head: {token.head.text})\")\n",
    "\n",
    "analyze_feedback(customer_feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25509942-a23e-49d9-add7-d19b4658e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261e4cee-a09e-45ac-bba3-d6edaa21bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your sentence (type 'exit' to end):  The world is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete Suggestions: ['Proposal', 'Hi', '[Recipient],', 'The', 'world', 'is', 'full', 'of', 'ideas.', \"I'm\", 'interested', 'in', 'your', 'ideas,', 'and', \"I'd\", 'like', 'to', 'hear', 'from', 'you', 'about', 'your', 'project.', 'Please', 'send', 'me', 'a', 'message', 'if', 'you', 'have', 'any', 'questions.']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your sentence (type 'exit' to end):  exit\n"
     ]
    }
   ],
   "source": [
    "#Case Study 2(Word Generation)\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class EmailAutocompleteSystem:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    def generate_suggestions(self, user_input, context):\n",
    "        input_text = f\"{context} {user_input}\"\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return generated_text[len(input_text):]  # This will give you only the generated part.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    autocomplete_system = EmailAutocompleteSystem()\n",
    "    email_context = \"Subject: Discussing Project Proposal\\nHi [Recipient],\"\n",
    "    while True:\n",
    "        user_input = input(\"Enter your sentence (type 'exit' to end): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        suggestions = autocomplete_system.generate_suggestions(user_input, email_context)\n",
    "        print(\"Autocomplete Suggestions:\", suggestions.strip() if suggestions.strip() else \"No suggestions available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da0efbf-fcdb-4339-9494-19d93c367bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.ibm.pc.hardware       0.92      0.92      0.92       183\n",
      "   comp.sys.mac.hardware       0.93      0.92      0.93       205\n",
      "               rec.autos       0.97      0.97      0.97       210\n",
      "         rec.motorcycles       0.97      0.98      0.97       189\n",
      "\n",
      "                accuracy                           0.95       787\n",
      "               macro avg       0.95      0.95      0.95       787\n",
      "            weighted avg       0.95      0.95      0.95       787\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Case Study-3(Text Classification)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset and split\n",
    "categories = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize data\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train and predict\n",
    "classifier = LinearSVC().fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {classifier.score(X_test, y_test):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49812809-19d2-4c96-a194-35dc471684a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Query: I received a damaged product. Can I get a refund?\n",
      "Semantic Analysis (Synonyms): ['draw', 'bugger_off', 'encounter', 'stupefy', 'obtain', 'engender', 'contract', 'pay_off', 'start_out', 'ware', 'grow', 'incur', 'start', 'merchandise', 'scram', 'damaged', 'bewilder', 'sire', 'meet', 'find', 'experience', 'product', 'take_in', 'acquire', 'buzz_off', 'damage', 'set_out', 'bring', 'fetch', 'have', 'discredited', 'return', 'baffle', 'amaze', 'refund', 'suffer', 'welcome', 'repay', 'develop', 'puzzle', 'induce', 'catch', 'stimulate', 'gravel', 'arrest', 'fuck_off', 'bring_forth', 'intersection', 'vex', 'receive', 'repayment', 'begin', 'let', 'production', 'sustain', 'mystify', 'make', 'arrive', 'beget', 'dumbfound', 'pay_back', 'cause', 'capture', 'Cartesian_product', 'go', 'pose', 'take', 'pick_up', 'aim', 'stick', 'become', 'invite', 'produce', 'get', 'flummox', 'come', 'fix', 'give_back', 'drive', 'beat', 'mother', \"get_under_one's_skin\", 'nonplus', 'set_about', 'convey', 'received', 'mathematical_product', 'get_down', 'generate', 'perplex', 'standard', 'father', 'commence']\n",
      "\n",
      "\n",
      "Customer Query: I'm having trouble accessing my account.\n",
      "Semantic Analysis (Synonyms): ['get_at', 'chronicle', 'account', 'trouble', 'worry', 'business_relationship', 'disturb', 'news_report', 'invoice', 'disorder', 'bother', 'hassle', 'score', 'inconvenience', 'access', 'describe', 'disoblige', 'discommode', 'inconvenience_oneself', 'history', 'story', 'unhinge', 'explanation', 'trouble_oneself', 'distract', 'account_statement', 'write_up', 'fuss', 'upset', 'accounting', 'ail', 'disquiet', 'cark', 'answer_for', 'problem', 'put_out', 'calculate', 'incommode', 'perturb', 'report', 'pain', 'difficulty', 'bill']\n",
      "\n",
      "\n",
      "Customer Query: How can I track my order status?\n",
      "Semantic Analysis (Synonyms): ['social_club', 'govern', 'set_up', 'trail', 'cut_across', 'range', 'society', 'get_across', 'grade', 'ordination', 'cross', 'chase', 'position', 'cover', 'fiat', 'monastic_order', 'cut_through', 'dog', 'tail', 'regularize', 'rate', 'data_track', 'status', 'guild', 'edict', 'course', 'say', 'running', 'regulate', 'put', 'ordinate', 'parliamentary_procedure', 'rails', 'place', 'caterpillar_track', 'Holy_Order', 'racecourse', 'give_chase', 'regularise', 'orderliness', 'raceway', 'lodge', 'order_of_magnitude', 'purchase_order', 'track', 'enjoin', 'get_over', 'rail', 'decree', 'cut', 'cartroad', 'gild', 'runway', 'Order', 'ordain', 'dictate', 'caterpillar_tread', 'club', 'cart_track', 'tell', 'condition', 'racetrack', 'rescript', 'go_after', 'traverse', 'arrange', 'pass_over', 'parliamentary_law', 'consecrate', 'rank', 'lead', 'prescribe', 'chase_after', 'path', 'tag', 'order', 'rules_of_order', 'ordering']\n",
      "\n",
      "\n",
      "Customer Query: The item I received doesn't match the description.\n",
      "Semantic Analysis (Synonyms): ['match', 'twin', 'encounter', 'play_off', 'obtain', 'tally', 'token', 'correspond', 'mates', 'pair', 'jibe', 'rival', 'equalize', 'lucifer', 'friction_match', 'description', 'compeer', 'point', 'incur', 'welcome', 'equate', 'cope_with', 'equal', 'invite', 'peer', 'verbal_description', 'oppose', 'mate', 'item', 'meet', 'touch', 'find', 'equalise', 'experience', 'catch', 'take_in', 'pick_up', 'check', 'couple', 'received', 'fit', 'detail', 'have', 'pit', 'receive', 'agree', 'standard', 'particular', 'gibe', 'get']\n",
      "\n",
      "\n",
      "Customer Query: Is there a discount available for bulk orders?\n",
      "Semantic Analysis (Synonyms): ['dictate', 'social_club', 'place', 'club', 'disregard', 'volume', 'monastic_order', 'govern', 'brush_off', 'Holy_Order', 'tell', 'set_up', 'majority', 'regularise', 'orderliness', 'rescript', 'bulk', 'lodge', 'deduction', 'order_of_magnitude', 'purchase_order', 'available', 'enjoin', 'usable', 'ignore', 'society', 'regularize', 'ordinate', 'arrange', 'range', 'grade', 'brush_aside', 'decree', 'ordination', 'rate', 'parliamentary_law', 'consecrate', 'ordain', 'mass', 'bulge', 'rank', 'discount_rate', 'guild', 'prescribe', 'edict', 'price_reduction', 'bank_discount', 'gild', 'useable', 'say', 'rebate', 'order', 'dismiss', 'rules_of_order', 'Order', 'fiat', 'ordering', 'regulate', 'put', 'push_aside', 'uncommitted', 'discount', 'parliamentary_procedure']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Case Study 4(Semantic_Analysis)\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to perform semantic analysis\n",
    "def semantic_analysis(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    synonyms = set()\n",
    "    for token in lemmatized_tokens:\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Example customer queries\n",
    "customer_queries = [\n",
    "    \"I received a damaged product. Can I get a refund?\",\n",
    "    \"I'm having trouble accessing my account.\",\n",
    "    \"How can I track my order status?\",\n",
    "    \"The item I received doesn't match the description.\",\n",
    "    \"Is there a discount available for bulk orders?\"\n",
    "]\n",
    "\n",
    "# Semantic analysis for each query\n",
    "for query in customer_queries:\n",
    "    print(\"Customer Query:\", query)\n",
    "    synonyms = semantic_analysis(query)\n",
    "    print(\"Semantic Analysis (Synonyms):\", synonyms)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277c068a-b064-4370-9c31-201fdc993637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This product is amazing! I love it.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: The product was good, but the packaging was damaged.\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: Very disappointing experience. Would not recommend.\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: Neutral feedback on the product.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Case Study 5(Sentiment Analysis)\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources (only required once)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Sample reviews\n",
    "reviews = [\n",
    "    \"This product is amazing! I love it.\",\n",
    "    \"The product was good, but the packaging was damaged.\",\n",
    "    \"Very disappointing experience. Would not recommend.\",\n",
    "    \"Neutral feedback on the product.\",\n",
    "]\n",
    "\n",
    "# Initialize Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment for each review\n",
    "for review in reviews:\n",
    "    print(\"Review:\", review)\n",
    "    scores = sid.polarity_scores(review)\n",
    "    print(\"Sentiment:\", end=' ')\n",
    "    if scores['compound'] > 0.05:\n",
    "        print(\"Positive\")\n",
    "    elif scores['compound'] < -0.05:\n",
    "        print(\"Negative\")\n",
    "    else:\n",
    "        print(\"Neutral\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a23063-581b-4e65-8e81-7f2b3641ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article Text:\n",
      " Manchester United secured a 3-1 victory over Chelsea in yesterday's match.\n",
      "    Goals from Rashford, Greenwood, and Fernandes sealed the win for United.\n",
      "    Chelsea's only goal came from Pulisic in the first half.\n",
      "    The victory boosts United's chances in the Premier League title race.\n",
      "    \n",
      "\n",
      "Parts of Speech Tagging:\n",
      "Manchester: NNP\n",
      "United: NNP\n",
      "secured: VBD\n",
      "a: DT\n",
      "3-1: JJ\n",
      "victory: NN\n",
      "over: IN\n",
      "Chelsea: NNP\n",
      "in: IN\n",
      "yesterday: NN\n",
      "'s: POS\n",
      "match: NN\n",
      ".: .\n",
      "Goals: NNS\n",
      "from: IN\n",
      "Rashford: NNP\n",
      ",: ,\n",
      "Greenwood: NNP\n",
      ",: ,\n",
      "and: CC\n",
      "Fernandes: NNP\n",
      "sealed: VBD\n",
      "the: DT\n",
      "win: NN\n",
      "for: IN\n",
      "United: NNP\n",
      ".: .\n",
      "Chelsea: NN\n",
      "'s: POS\n",
      "only: JJ\n",
      "goal: NN\n",
      "came: VBD\n",
      "from: IN\n",
      "Pulisic: NNP\n",
      "in: IN\n",
      "the: DT\n",
      "first: JJ\n",
      "half: NN\n",
      ".: .\n",
      "The: DT\n",
      "victory: NN\n",
      "boosts: VBZ\n",
      "United: NNP\n",
      "'s: POS\n",
      "chances: NNS\n",
      "in: IN\n",
      "the: DT\n",
      "Premier: NNP\n",
      "League: NNP\n",
      "title: NN\n",
      "race: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "#Case Study 6(POS Tagging)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tagged_tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tagged_tokens.extend(nltk.pos_tag(tokens))\n",
    "    return tagged_tokens\n",
    "\n",
    "def main():\n",
    "    article_text = \"\"\"Manchester United secured a 3-1 victory over Chelsea in yesterday's match.\n",
    "    Goals from Rashford, Greenwood, and Fernandes sealed the win for United.\n",
    "    Chelsea's only goal came from Pulisic in the first half.\n",
    "    The victory boosts United's chances in the Premier League title race.\n",
    "    \"\"\"\n",
    "    tagged_tokens = pos_tagging(article_text)\n",
    "    print(\"Original Article Text:\\n\", article_text)\n",
    "    print(\"\\nParts of Speech Tagging:\")\n",
    "    for token, pos_tag in tagged_tokens:\n",
    "        print(f\"{token}: {pos_tag}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b786f8-dd73-441e-99aa-c0e0db2ac411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Noun Phrases:\n",
      "- The quick brown\n",
      "- fox\n",
      "- the lazy dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Case Study 7(Chunking)\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set NLTK data path\n",
    "nltk.data.path.append(\"/usr/local/share/nltk_data\")\n",
    "\n",
    "# Download the 'punkt' tokenizer model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the 'averaged_perceptron_tagger' model\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Define chunk grammar\n",
    "chunk_grammar = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} # Chunk sequences of DT, JJ, NN\n",
    "\"\"\"\n",
    "\n",
    "# Create chunk parser\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Apply chunking\n",
    "chunked_text = chunk_parser.parse(pos_tags)\n",
    "\n",
    "# Extract noun phrases\n",
    "noun_phrases = []\n",
    "for subtree in chunked_text.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "    noun_phrases.append(' '.join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "# Output\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Noun Phrases:\")\n",
    "for phrase in noun_phrases:\n",
    "    print(\"-\", phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b23d06-36ab-40e1-adb1-01cbd5b151ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
